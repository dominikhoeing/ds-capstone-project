{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "### \"Capybaras\" capstone image recognition project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install PIL - tool for image processing.\n",
    "#!pip install pillow\n",
    "\n",
    "Import all necessary tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m output_test_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/new_test_features\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load labels from CSV\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39mfrac, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Load a random fraction of the data\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train_labels.csv'"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "dataset_folder = '../data/new_train_features'  # Path to the folder with images\n",
    "labels_file = '../data/train_labels.csv'  # Path to the CSV file with labels\n",
    "\n",
    "# Path to the train folder and the new train folder\n",
    "input_train_folder = '../data/train_features'\n",
    "output_train_folder = '../data/new_train_features'\n",
    "\n",
    "# Path to the test folder and the new test folder\n",
    "input_test_folder = '../data/test_features'\n",
    "output_test_folder = '../data/new_test_features'\n",
    "\n",
    "# Load labels from CSV\n",
    "df = pd.read_csv(labels_file)\n",
    "\n",
    "frac = 1.0\n",
    "\n",
    "df = df.sample(frac=frac, random_state=1) # Load a random fraction of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess the data. \n",
    "Below we defined the functions to preprocess data for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sizes(input_folder):\n",
    "    \"\"\"Calculates the maximum width and height of all images in the input folder.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing the images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Maximum width and height of the images found in the folder.\n",
    "    \"\"\"\n",
    "    # Initialize maximum width and height\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    \n",
    "    # Iterate through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # Check if the file is an image with a supported extension\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            # Open the image file\n",
    "            with Image.open(file_path) as img:\n",
    "                # Get the width and height of the image\n",
    "                width, height = img.size\n",
    "                # Update max_width if the current image's width is greater\n",
    "                if width > max_width:\n",
    "                    max_width = width\n",
    "                # Update max_height if the current image's height is greater\n",
    "                if height > max_height:\n",
    "                    max_height = height\n",
    "    # Return the maximum width and height found\n",
    "    return max_width, max_height\n",
    "\n",
    "# Define the range of white color\n",
    "def is_white_color(color, threshold=230):\n",
    "    # Check if color is close to white within the given threshold\n",
    "    return all(c >= threshold for c in color)\n",
    "\n",
    "\n",
    "# Adjust threshold based on what is considered \"close to white\"\n",
    "def crop_white_bottom_from_image(img, height_to_check=8, check_width=10, start_from=60, threshold=230):\n",
    "    width, height = img.size\n",
    "    # Define the region to check for white color\n",
    "    box = (start_from, height - height_to_check, start_from + check_width, height)\n",
    "    region = img.crop(box)\n",
    "    # Convert the region to a NumPy array for easy processing\n",
    "    region_np = np.array(region)\n",
    "    # Check if all pixels in the region are close to white\n",
    "    white_pixels = np.apply_along_axis(is_white_color, 1, region_np)\n",
    "    \n",
    "    if np.all(white_pixels):\n",
    "        # Crop the bottom 16 pixels\n",
    "        img = img.crop((0, 0, width, height - height_to_check - 8))\n",
    "    return img\n",
    "\n",
    "\n",
    "def padding_images(img,max_width,max_height):\n",
    "    # Calculate padding required to center the image\n",
    "    delta_width = max_width - img.width\n",
    "    delta_height = max_height - img.height\n",
    "    padding = (delta_width // 2, delta_height // 2, delta_width - (delta_width // 2), delta_height - (delta_height // 2))\n",
    "    # Add padding and create a new image with the desired size\n",
    "    return ImageOps.expand(img, padding, fill=0)\n",
    "\n",
    "def format_to_rgb(img):\n",
    "    if img.mode != 'RGB':\n",
    "        # Convert image to RGB\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "def crop_white_bottom_add_padding_all_rgb(input_folder,output_folder):\n",
    "    max_width, max_height = max_sizes(input_folder)\n",
    "\n",
    "    # Create the new folder if it does not exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # Iterate through each image in the source folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            with Image.open(file_path) as img:\n",
    "                img = crop_white_bottom_from_image(img)\n",
    "                img = padding_images(img,max_width,max_height)\n",
    "                img = format_to_rgb(img)\n",
    "                # Save the original image to the new folder\n",
    "                new_file_path = os.path.join(output_folder, filename)\n",
    "                img.save(new_file_path)\n",
    "    print(output_folder, \"Processing complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual preprocessing\n",
    "crop_white_bottom_add_padding_all_rgb(input_train_folder,output_train_folder)\n",
    "crop_white_bottom_add_padding_all_rgb(input_test_folder,output_test_folder)\n",
    "\n",
    "# Image parameters\n",
    "img_width, img_height = max_sizes(dataset_folder)\n",
    "#img_height = 128  # Set your image height\n",
    "#img_width = 128   # Set your image width\n",
    "batch_size = 32\n",
    "\n",
    "# Add a full path to each image in the dataframe\n",
    "df['id'] = df['id'].apply(lambda x: os.path.join(dataset_folder, x+'.jpg'))\n",
    "\n",
    "# Data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # Step 1: Normalize pixel values to the range [0, 1]\n",
    "\n",
    "# Create the training generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,  # Step 2: Use training DataFrame\n",
    "    directory=None,  # Images paths are specified in the DataFrame\n",
    "    x_col='id',  # Column with image file paths\n",
    "    y_col=train_df.columns[1:],  # All columns except 'ID' are considered as labels\n",
    "    target_size=(img_height, img_width),  # Step 3: Resize images to specified dimensions\n",
    "    batch_size=batch_size,  # Step 4: Number of images to process in each batch\n",
    "    class_mode='raw'  # Step 5: Output labels as they are, without any transformation\n",
    ")\n",
    "\n",
    "# Create the validation generator\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,  # Step 6: Use validation DataFrame\n",
    "    directory=None,  # Images paths are specified in the DataFrame\n",
    "    x_col='id',  # Column with image file paths\n",
    "    y_col=val_df.columns[1:],  # All columns except 'ID' are considered as labels\n",
    "    target_size=(img_height, img_width),  # Step 7: Resize images to specified dimensions\n",
    "    batch_size=batch_size,  # Step 8: Number of images to process in each batch\n",
    "    class_mode='raw'  # Step 9: Output labels as they are, without any transformation\n",
    ")\n",
    "\n",
    "\"\"\"# Example: Display a batch of images and labels\n",
    "images, labels = next(train_generator)  # Step 10: Get the next batch of images and labels from the training generator\n",
    "plt.figure(figsize=(12, 12))  # Step 11: Create a new figure for plotting\n",
    "for i in range(9):  # Step 12: Loop over the first 9 images in the batch\n",
    "    plt.subplot(3, 3, i + 1)  # Step 13: Define a 3x3 grid of subplots\n",
    "    plt.imshow(images[i])  # Step 14: Display the i-th image\n",
    "    plt.title(f\"Labels: {labels[i]}\")  # Step 15: Add title with corresponding labels\n",
    "    plt.axis('off')  # Step 16: Hide the axis\n",
    "plt.show()  # Step 17: Show the plot with images\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train/Test-split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified split based on the labels\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[df.columns[1:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(len(df.columns) - 1, activation='softmax')  # Output layer for multi-label classification\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x175dd7050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 538, 958, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 269, 479, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 267, 477, 16)      2320      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 133, 238, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 506464)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                8103440   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8106344 (30.92 MB)\n",
      "Trainable params: 8106344 (30.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the saved model\n",
    "with tf.device('/cpu:0'):\n",
    "    new_model = tf.keras.models.load_model('/Users/alexandersimakov/Documents/CAPSTONE/ds-capstone-project/models/firstCNNmodel')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluation\n",
    "\n",
    "6.1 Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m new_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model = new_model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_generator\u001b[49m, validation_data\u001b[38;5;241m=\u001b[39mval_generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the accuracy for each epoch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy for each epoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the accuracy for each epoch\n",
    "print(\"Training accuracy for each epoch:\", history.history['accuracy'])\n",
    "print(\"Validation accuracy for each epoch:\", history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metric(model, saving = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots and (optionally) saves accuracy and loss of the model per epoch.\n",
    "    Args:\n",
    "        model: The trained model object\n",
    "        saving = False: if True, it asks for user input of a filename. The accuracies and losses are saved as a csv file in the current directory.\n",
    "    Returns:\n",
    "        plots of Training and Validation Accuracy\n",
    "        saves result as csv file (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    # create variable for different accuracy metrics vs. epochs\n",
    "    accuracy = model.history['accuracy']\n",
    "    val_accuracy = model.history['val_accuracy']\n",
    "    loss = model.history['loss']\n",
    "    val_loss = model.history['val_loss']\n",
    "\n",
    "    # plot Accuracy\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.plot(accuracy,label='train_accuracy')\n",
    "    ax1.plot(val_accuracy,label='val_accuracy')\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.legend()\n",
    "    fig1.show()\n",
    "\n",
    "    # plot Loss\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.plot(loss,label='train_loss')\n",
    "    ax2.plot(val_loss,label='val_loss')\n",
    "    ax2.set_xlabel('epochs')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.legend()\n",
    "    fig2.show()\n",
    "\n",
    "    # saving the metrics in a csv-file\n",
    "    if saving == True:\n",
    "        d = {'accuracy': accuracy, 'val_accuracy': val_accuracy, 'loss': loss, 'val_loss': val_loss}\n",
    "        df = pd.DataFrame(d)\n",
    "        name = input(\"Please enter the filename (without .csv): \")\n",
    "        name = name + '.csv'\n",
    "        df.to_csv(name)\n",
    "\n",
    "evaluation_metric(history, saving = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Get Predictions on Validation Data\n",
    "val_preds = model.predict(val_generator)\n",
    "\n",
    "\n",
    "# Convert predictions to class labels\n",
    "val_preds_classes = np.argmax(val_preds, axis=1)\n",
    "\n",
    "# Step 2: Get True Labels\n",
    "# Extract true labels from the validation generator\n",
    "val_true_labels = val_generator.labels\n",
    "\n",
    "# Manually specify class labels\n",
    "class_labels = ['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']  # Adjust as necessary\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(val_true_labels, val_preds_classes)\n",
    "\n",
    "# Display the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels).plot(ax=ax, xticks_rotation=90, colorbar=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
